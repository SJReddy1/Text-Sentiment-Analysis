{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankirnajoshi/sentiment-app/blob/master/sentiment_app_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zpnxxaB_MAhs"
      },
      "source": [
        "# Sentiment-app model\n",
        "This notebook is used to train the LSTM model for the sentiment app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Vc0BpuNctP3"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "colab_type": "code",
        "id": "FVk3fIa_icG3",
        "outputId": "1c3c2837-b3fb-4186-83d8-eec7fdb65862"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import sys\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "#from nltk.corpus import  stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "url = 'http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip'\n",
        "\n",
        "def download_url(url, save_path):\n",
        "    '''\n",
        "    Download the zip file from stanford website\n",
        "    '''\n",
        "    with urllib.request.urlopen(url) as dl_file:\n",
        "        with open(save_path, 'wb') as out_file:\n",
        "            out_file.write(dl_file.read())\n",
        "\n",
        "\n",
        "def dataset_preparation(filename, header=None):\n",
        "    '''\n",
        "    Extract the downloaded zip with the folder structure intact. \n",
        "    ##To Do - optimize the function to remove folder structure and download only the required files.\n",
        "    '''\n",
        "    data = pd.read_csv(filename, sep=\"|\", header=header)\n",
        "    return data    \n",
        "\n",
        "\n",
        "def target_map(target):\n",
        "    '''\n",
        "    Apply function to target to get ratings in \n",
        "    desired form. Ratings will be converted from \n",
        "    [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
        "    to [0,1,2,3,4]. The new ratings are defined as :\n",
        "    0 - very negative\n",
        "    1 - somewhat negative\n",
        "    2 - neutral\n",
        "    3 - somewhat positive\n",
        "    4 - very positive\n",
        "    '''\n",
        "    if 0 <= target <= 0.2:\n",
        "        return 0\n",
        "    elif 0.2 < target <=0.4:\n",
        "        return 1\n",
        "    elif 0.4 < target <=0.6:\n",
        "        return 2\n",
        "    elif 0.6 < target <=0.8:\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "\n",
        "def clean_data(df, col):\n",
        "    ##Remove records without any words\n",
        "    #df[col] = pd.Series([BeautifulSoup(text).get_text() for text in df[col]])\n",
        "    df[col] = df[df[col].str.contains('[A-Za-z]')] #remove rows without any alphabets.\n",
        "    df[col] = df[col].str.replace('[^\\w\\s]','').str.lower().str.strip() #replace punctuations, remove case and strip whitespaces\n",
        "    df[col] = df[col].replace('', np.NaN)\n",
        "    df = df.drop_duplicates(subset=[col])\n",
        "    df = df.dropna(subset=[col])\n",
        "    df[col] = df[col].dropna()\n",
        "    df[col] = df[col].apply(lambda x: word_tokenize(x))\n",
        "    df[col] = df[col].apply(lambda x : [lemmatizer.lemmatize(item) for item in x])\n",
        "    return df\n",
        "\n",
        "if not path.exists('../data'):\n",
        "    download_url(url,'../data.zip')\n",
        "    with zipfile.ZipFile('../data.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('../data/')\n",
        "\n",
        "run = False\n",
        "if run:\n",
        "    dictionary = dataset_preparation('../data/stanfordSentimentTreebank/dictionary.txt')\n",
        "    dictionary.columns = ['phrase','phrase_id']\n",
        "\n",
        "    sentiment = dataset_preparation('../data/stanfordSentimentTreebank/sentiment_labels.txt', header=0)\n",
        "    sentiment.columns=['phrase_id','target']\n",
        "\n",
        "    data = pd.merge(dictionary, sentiment, how='inner',on='phrase_id')\n",
        "\n",
        "    data['target'] = data['target'].map(target_map)\n",
        "    df = clean_data(data, 'phrase')\n",
        "    df.to_pickle('../model/merged_cleaned_data.pkl')\n",
        "    del data, dictionary, sentiment;\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM,Flatten,MaxPooling1D,Conv1D,Bidirectional\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import  categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#set random seed for the session and also for tensorflow that runs in background for keras\n",
        "import random\n",
        "tf.random.set_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df.phrase\n",
        "y = df.target\n",
        "y = to_categorical(y) #one hot encode y\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=0)\n",
        "\n",
        "del X_train_val, y_train_val;\n",
        "\n",
        "unique_words = set(np.hstack(X_train))\n",
        "print(f'Unique words : {len(unique_words)}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Summarize review length\n",
        "print(\"Mean length: \")\n",
        "result = [len(x) for x in X_train]\n",
        "print(f'{np.mean(result):.2f} +- {np.std(result):.2f} words')\n",
        "# plot review length\n",
        "plt.boxplot(result)\n",
        "plt.show();\n",
        "\n",
        "MAX_LENGTH = max(map(len, X_train))\n",
        "print(\"max length:\", MAX_LENGTH)\n",
        "\n",
        "plt.title('Sequence length distribution')\n",
        "plt.hist(list(map(len, X_train)), bins=25);\n",
        "\n",
        "tokenizer = Tokenizer(num_words=len(list(unique_words)),oov_token='#')\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(list(X_train))\n",
        "X_val = tokenizer.texts_to_sequences(list(X_val))\n",
        "X_test = tokenizer.texts_to_sequences(list(X_test))\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=MAX_LENGTH)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=MAX_LENGTH)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
        "\n",
        "print(X_train.shape,X_val.shape,X_test.shape)\n",
        "print(y_train.shape, y_val.shape, y_test.shape)\n",
        "\n",
        "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_accuracy', patience = 2)\n",
        "callback = [early_stopping]\n",
        "\n",
        "num_classes = 5\n",
        "model=Sequential()\n",
        "model.add(Embedding(len(list(unique_words)),300,input_length=MAX_LENGTH))\n",
        "model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
        "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6, batch_size=256, verbose=1, callbacks=callback)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "# Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.\n",
        "#As mentioned before, altering different hyper parameters especially learning rate can have a positive impact\n",
        "#on accuracy and learning curve.\n",
        "plt.plot(epoch_count, history.history['loss'], 'r--')\n",
        "plt.plot(epoch_count, history.history['val_loss'], 'b-')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import model_from_json\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"../data/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"../data/model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"End of training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fn0LiieM56wQ"
      },
      "source": [
        "## Other Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "colab_type": "code",
        "id": "-mw18W4dcRGw",
        "outputId": "09faeb3d-a55b-48d7-d4f1-991dc3db6fa1"
      },
      "outputs": [],
      "source": [
        "## Model 2 Bidirectional LSTM\n",
        "num_classes = 5\n",
        "model_1=Sequential()\n",
        "model_1.add(Embedding(len(list(unique_words)),64,input_length=MAX_LENGTH))\n",
        "model_1.add(Bidirectional(LSTM(64,dropout=0.3, recurrent_dropout=0.3,return_sequences=True)))\n",
        "model_1.add(Dropout(0.3))\n",
        "model_1.add(Bidirectional(LSTM(64,dropout=0.5, recurrent_dropout=0.3,return_sequences=False)))\n",
        "model_1.add(Dropout(0.3))\n",
        "model_1.add(Dense(32,activation='relu'))\n",
        "model_1.add(Dropout(0.3))\n",
        "model_1.add(Dense(num_classes,activation='softmax'))\n",
        "model_1.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "colab_type": "code",
        "id": "-wj7w6pq5vMn",
        "outputId": "1756ee91-8e4c-4b94-eebe-18fcdccf071d"
      },
      "outputs": [],
      "source": [
        "history=model_1.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6, batch_size=256, verbose=1, callbacks=callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "colab_type": "code",
        "id": "ZmWUdZYU8CGy",
        "outputId": "d1d21106-e174-4b88-d042-3a798bba0677"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "# Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.\n",
        "#As mentioned before, altering different hyper parameters especially learning rate can have a positive impact\n",
        "#on accuracy and learning curve.\n",
        "plt.plot(epoch_count, history.history['loss'], 'r--')\n",
        "plt.plot(epoch_count, history.history['val_loss'], 'b-')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "scores = model_1.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOsAlrgbVPeK5m8RqpXUFMk",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "sentiment-app-training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3dc971442b6bc7eb52a5d5fe5eadecc7885b8b2c4d67498a996a4ea4838d4216"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
